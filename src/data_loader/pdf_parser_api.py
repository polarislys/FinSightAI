"""
PDF è§£æå™¨ - ä½¿ç”¨ MinerU å®˜æ–¹ API + cninfo URL åæŸ¥
"""
import requests
import time
import zipfile
import io
from pathlib import Path
from typing import Optional, Dict, List
import logging
import os
from dotenv import load_dotenv

load_dotenv()
logger = logging.getLogger(__name__)

# cninfo é…ç½®
CNINFO_API = "http://www.cninfo.com.cn/new/hisAnnouncement/query"
CNINFO_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
    "Accept": "application/json, text/javascript, */*; q=0.01",
    "Referer": "http://www.cninfo.com.cn/new/disclosure",
}


class PDFParserAPI:
    """ä½¿ç”¨ MinerU å®˜æ–¹ API è§£æ PDF"""
    
    def __init__(
        self, 
        output_dir: str = "./data/processed",
        api_token: str = None,
        user_token: str = None
    ):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # MinerU API Token
        self.api_token = api_token or os.getenv("MINERU_API_TOKEN")
        self.user_token = user_token or os.getenv("MINERU_USER_TOKEN", "default_user")
        
        if not self.api_token:
            raise ValueError("éœ€è¦è®¾ç½® MINERU_API_TOKEN ç¯å¢ƒå˜é‡")
        
        self.api_base_url = "https://mineru.net/api/v4/extract"
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_token}",
            "token": self.user_token
        }
        
        logger.info(f"âœ… MinerU API è§£æå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   API: {self.api_base_url}")
    
    def _resolve_cninfo_pdf_url(self, pdf_filename: str) -> Optional[str]:
        """
        ä»æ–‡ä»¶ååæŸ¥ cninfo è·å– PDF URL
        
        æ–‡ä»¶åæ ¼å¼: å…¬å¸ç®€ç§°_å…¬å‘Šæ ‡é¢˜.pdf
        ä¾‹å¦‚: ä¸­æ’ç”µæ°”_å…³äºä¸å…³è”äººå…±åŒæŠ•èµ„è®¾ç«‹åˆèµ„å…¬å¸...å…¬å‘Š.pdf
        """
        try:
            stem = Path(pdf_filename).stem
            
            # åˆ†ç¦»å…¬å¸ç®€ç§°å’Œå…¬å‘Šæ ‡é¢˜
            if '_' in stem:
                sec_name, title = stem.split('_', 1)
            else:
                sec_name = ""
                title = stem
            
            # æ¸…ç†æ ‡é¢˜ä¸­çš„ em æ ‡è®°
            title_clean = title.replace("em", "")
            
            logger.info(f"   ğŸ” åæŸ¥ URL: {sec_name} - {title_clean[:30]}...")
            
            # æœç´¢ cninfo
            payload = {
                "pageNum": 1,
                "pageSize": 10,
                "column": "szse",
                "tabName": "fulltext",
                "plate": "",
                "stock": "",
                "searchkey": title_clean[:50],  # ç”¨æ ‡é¢˜å‰50å­—ç¬¦æœç´¢
                "secid": "",
                "category": "",
                "trade": "",
                "seDate": "",
                "sortName": "",
                "sortType": "",
                "isHLtitle": "true",
            }
            
            resp = requests.post(CNINFO_API, data=payload, headers=CNINFO_HEADERS, timeout=30)
            resp.raise_for_status()
            data = resp.json()
            
            announcements = data.get("announcements", [])
            if not announcements:
                logger.warning(f"   âš ï¸  æœªæ‰¾åˆ°å…¬å‘Š: {title_clean[:30]}")
                return None
            
            # åŒ¹é…å…¬å¸ç®€ç§°
            for ann in announcements:
                ann_sec_name = ann.get("secName", "")
                ann_title = ann.get("announcementTitle", "")
                adj_url = ann.get("adjunctUrl")
                
                if not adj_url:
                    continue
                
                # æ¨¡ç³ŠåŒ¹é…ï¼šå…¬å¸ç®€ç§°ç›¸åŒæˆ–æ ‡é¢˜åŒ…å«å…³é”®è¯
                if sec_name and ann_sec_name == sec_name:
                    pdf_url = f"http://static.cninfo.com.cn/{adj_url}"
                    logger.info(f"   âœ… æ‰¾åˆ° URL: {pdf_url[:60]}...")
                    return pdf_url
            
            # å¦‚æœæ²¡æœ‰ç²¾ç¡®åŒ¹é…ï¼Œå–ç¬¬ä¸€ä¸ªç»“æœ
            first_ann = announcements[0]
            adj_url = first_ann.get("adjunctUrl")
            if adj_url:
                pdf_url = f"http://static.cninfo.com.cn/{adj_url}"
                logger.info(f"   âœ… ä½¿ç”¨ç¬¬ä¸€ä¸ªç»“æœ: {pdf_url[:60]}...")
                return pdf_url
            
            return None
            
        except Exception as e:
            logger.error(f"   âŒ åæŸ¥ URL å¤±è´¥: {e}")
            return None
    
    def _create_task(self, pdf_url: str) -> Optional[str]:
        """åˆ›å»º MinerU è§£æä»»åŠ¡"""
        try:
            task_data = {
                "url": pdf_url,
                "model_version": "vlm",
                "is_ocr": True
            }
            
            response = requests.post(
                f"{self.api_base_url}/task",
                headers=self.headers,
                json=task_data,
                timeout=30
            )
            
            if response.status_code != 200:
                logger.error(f"   âŒ åˆ›å»ºä»»åŠ¡å¤±è´¥: {response.text}")
                return None
            
            result = response.json()
            if result.get("code") != 0:
                logger.error(f"   âŒ API é”™è¯¯: {result.get('msg')}")
                return None
            
            task_id = result["data"]["task_id"]
            logger.info(f"   âœ… ä»»åŠ¡åˆ›å»ºæˆåŠŸ: {task_id}")
            return task_id
            
        except Exception as e:
            logger.error(f"   âŒ åˆ›å»ºä»»åŠ¡å¼‚å¸¸: {e}")
            return None
    
    def _poll_task(self, task_id: str, timeout: int = 300, poll_interval: int = 5) -> Optional[str]:
        """è½®è¯¢ä»»åŠ¡çŠ¶æ€ï¼Œè¿”å› zip URL"""
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            try:
                response = requests.get(
                    f"{self.api_base_url}/task/{task_id}",
                    headers=self.headers,
                    timeout=10
                )
                
                if response.status_code != 200:
                    logger.error(f"   âŒ æŸ¥è¯¢çŠ¶æ€å¤±è´¥: {response.text}")
                    return None
                
                result = response.json()
                if result.get("code") != 0:
                    logger.error(f"   âŒ API é”™è¯¯: {result.get('msg')}")
                    return None
                
                data = result["data"]
                state = data.get("state")
                
                if state == "done":
                    zip_url = data.get("full_zip_url")
                    logger.info(f"   âœ… è§£æå®Œæˆ")
                    return zip_url
                
                elif state == "failed":
                    error_msg = data.get("err_msg", "æœªçŸ¥é”™è¯¯")
                    logger.error(f"   âŒ è§£æå¤±è´¥: {error_msg}")
                    return None
                
                elif state == "running":
                    progress = data.get("extract_progress", {})
                    extracted = progress.get("extracted_pages", 0)
                    total = progress.get("total_pages", 0)
                    logger.info(f"   â³ è¿›åº¦: {extracted}/{total} é¡µ...")
                
                else:
                    logger.info(f"   â³ çŠ¶æ€: {state}...")
                
                time.sleep(poll_interval)
                
            except Exception as e:
                logger.error(f"   âŒ è½®è¯¢å¼‚å¸¸: {e}")
                time.sleep(poll_interval)
        
        logger.error(f"   âŒ è¶…æ—¶")
        return None
    
    def _download_and_extract(self, zip_url: str, pdf_name: str) -> Optional[str]:
        """ä¸‹è½½ zip å¹¶è§£å‹ï¼Œè¿”å› markdown è·¯å¾„"""
        try:
            logger.info(f"   ğŸ“¥ ä¸‹è½½ç»“æœ...")
            response = requests.get(zip_url, timeout=120)
            
            if response.status_code != 200:
                logger.error(f"   âŒ ä¸‹è½½å¤±è´¥: {response.status_code}")
                return None
            
            # è§£å‹
            output_dir = self.output_dir / pdf_name
            output_dir.mkdir(parents=True, exist_ok=True)
            
            with zipfile.ZipFile(io.BytesIO(response.content)) as zf:
                zf.extractall(output_dir)
            
            # æŸ¥æ‰¾ markdown æ–‡ä»¶
            md_files = list(output_dir.rglob("*.md"))
            if not md_files:
                logger.error(f"   âŒ æœªæ‰¾åˆ° Markdown æ–‡ä»¶")
                return None
            
            md_path = md_files[0]
            logger.info(f"   ğŸ“„ Markdown: {md_path.name}")
            return str(md_path)
            
        except Exception as e:
            logger.error(f"   âŒ ä¸‹è½½è§£å‹å¼‚å¸¸: {e}")
            return None
    
    def parse(self, pdf_path: str) -> Optional[Dict[str, str]]:
        """è§£æå•ä¸ª PDF"""
        try:
            pdf_path = Path(pdf_path)
            pdf_name = pdf_path.stem
            
            logger.info(f"ğŸ“„ è§£æ: {pdf_name[:40]}...")
            
            # 1. åæŸ¥ URL
            pdf_url = self._resolve_cninfo_pdf_url(pdf_path.name)
            if not pdf_url:
                logger.error(f"   âŒ æ— æ³•è·å– PDF URL")
                return None
            
            # 2. åˆ›å»ºä»»åŠ¡
            task_id = self._create_task(pdf_url)
            if not task_id:
                return None
            
            # 3. è½®è¯¢çŠ¶æ€
            zip_url = self._poll_task(task_id)
            if not zip_url:
                return None
            
            # 4. ä¸‹è½½è§£å‹
            md_path = self._download_and_extract(zip_url, pdf_name)
            if not md_path:
                return None
            
            # è¯»å–å†…å®¹
            content = Path(md_path).read_text(encoding='utf-8')
            images_dir = Path(md_path).parent / "images"
            
            return {
                'markdown': md_path,
                'images': str(images_dir) if images_dir.exists() else None,
                'content': content
            }
            
        except Exception as e:
            logger.error(f"âŒ å¼‚å¸¸: {pdf_path} - {e}")
            return None
    
    def batch_parse(
        self, 
        pdf_dir: str, 
        skip_existing: bool = True
    ) -> List[Dict[str, str]]:
        """æ‰¹é‡è§£æ PDF"""
        pdf_files = list(Path(pdf_dir).glob("*.pdf"))
        logger.info(f"ğŸ“š æ‰¾åˆ° {len(pdf_files)} ä¸ª PDF æ–‡ä»¶")
        
        parsed_results = []
        
        for idx, pdf_path in enumerate(pdf_files, 1):
            pdf_name = pdf_path.stem
            
            # æ£€æŸ¥æ˜¯å¦å·²è§£æ
            if skip_existing:
                existing_md = list((self.output_dir / pdf_name).rglob("*.md"))
                if existing_md:
                    logger.info(f"â­ï¸  [{idx}/{len(pdf_files)}] è·³è¿‡å·²è§£æ: {pdf_name[:30]}...")
                    parsed_results.append({
                        'markdown': str(existing_md[0]),
                        'images': str(existing_md[0].parent / "images"),
                        'content': existing_md[0].read_text(encoding='utf-8')
                    })
                    continue
            
            # è§£æ
            logger.info(f"\nğŸ“„ [{idx}/{len(pdf_files)}] è§£æ: {pdf_name[:40]}...")
            result = self.parse(str(pdf_path))
            
            if result:
                parsed_results.append(result)
            
            # å»¶è¿Ÿï¼Œé¿å… API é™æµ
            if idx < len(pdf_files):
                time.sleep(2)
        
        logger.info(f"\nâœ… æ‰¹é‡è§£æå®Œæˆ: {len(parsed_results)}/{len(pdf_files)} ä¸ª")
        return parsed_results
"""
æ–‡æœ¬åˆ‡åˆ†å¤„ç†å™¨ - è´Ÿè´£å°†è§£æåçš„æ–‡æ¡£åˆ‡åˆ†æˆæ–‡æœ¬å—
"""
from pathlib import Path
from typing import List, Dict
import logging

from src.data_loader.text_splitter import FinancialTextSplitter

logger = logging.getLogger(__name__)


class ChunkProcessor:
    """æ–‡æœ¬åˆ‡åˆ†å¤„ç†å™¨"""
    
    def __init__(self, chunk_size: int = 512):
        """
        åˆå§‹åŒ–åˆ‡åˆ†å¤„ç†å™¨
        
        Args:
            chunk_size: æ¯ä¸ªæ–‡æœ¬å—çš„æœ€å¤§ token æ•°
        """
        self.text_splitter = FinancialTextSplitter(chunk_size=chunk_size)
        logger.info(f"âœ… ChunkProcessor åˆå§‹åŒ–å®Œæˆ (chunk_size={chunk_size})")
    
    def process_parsed_results(self, parsed_results: List[Dict]) -> List[Dict]:
        """
        å¤„ç†è§£æç»“æœï¼Œåˆ‡åˆ†æˆæ–‡æœ¬å—
        
        Args:
            parsed_results: PDF è§£æç»“æœåˆ—è¡¨
                [{'markdown': 'path.md', 'images': 'path/images', ...}, ...]
        
        Returns:
            æ–‡æœ¬å—åˆ—è¡¨
                [{'text': str, 'metadata': dict, 'chunk_id': int}, ...]
        """
        logger.info(f"\n{'='*60}")
        logger.info("âœ‚ï¸  æ–‡æœ¬åˆ‡åˆ†å¤„ç†")
        logger.info(f"{'='*60}")
        
        if not parsed_results:
            logger.warning("âš ï¸  æ²¡æœ‰è§£æç»“æœéœ€è¦å¤„ç†")
            return []
        
        all_chunks = []
        
        for idx, result in enumerate(parsed_results, 1):
            md_path = result['markdown']
            source_name = Path(md_path).name
            
            # è¯»å– Markdown æ–‡ä»¶
            try:
                with open(md_path, 'r', encoding='utf-8') as f:
                    text = f.read()
            except Exception as e:
                logger.error(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {md_path} - {e}")
                continue
            
            # åˆ‡åˆ†æ–‡æœ¬
            chunks = self.text_splitter.split_text(text)
            
            # è®°å½•åˆ‡åˆ†ä¿¡æ¯
            logger.info(f"\nğŸ“„ [{idx}/{len(parsed_results)}] {source_name}")
            logger.info(f"   - åŸå§‹æ–‡æœ¬é•¿åº¦: {len(text):,} å­—ç¬¦")
            logger.info(f"   - åˆ‡åˆ†åå—æ•°: {len(chunks)} ä¸ª")
            
            # æ˜¾ç¤ºå‰ 2 ä¸ªå—çš„é¢„è§ˆ
            for i, chunk in enumerate(chunks[:2]):
                preview = chunk[:100].replace('\n', ' ')
                logger.info(f"   - Chunk {i}: {preview}...")
            
            if len(chunks) > 2:
                logger.info(f"   - ... è¿˜æœ‰ {len(chunks) - 2} ä¸ªå—")
            
            # æ·»åŠ åˆ°æ€»åˆ—è¡¨
            for i, chunk in enumerate(chunks):
                all_chunks.append({
                    'text': chunk,
                    'metadata': {
                        'source': source_name,
                        'doc_index': idx - 1
                    },
                    'chunk_id': i
                })
        
        # ç»Ÿè®¡ä¿¡æ¯
        logger.info(f"\n{'='*60}")
        logger.info("âœ… æ–‡æœ¬åˆ‡åˆ†å®Œæˆ")
        logger.info(f"{'='*60}")
        logger.info(f"   - å¤„ç†æ–‡æ¡£: {len(parsed_results)} ä¸ª")
        logger.info(f"   - æ€»åˆ‡ç‰‡æ•°: {len(all_chunks)} ä¸ª")
        if parsed_results:
            logger.info(f"   - å¹³å‡æ¯æ–‡æ¡£: {len(all_chunks) / len(parsed_results):.1f} ä¸ªåˆ‡ç‰‡")
        logger.info(f"{'='*60}\n")
        
        return all_chunks
    
    def save_chunks_to_file(self, chunks: List[Dict], output_path: str):
        """
        å°†åˆ‡ç‰‡ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆå¯é€‰åŠŸèƒ½ï¼Œç”¨äºè°ƒè¯•ï¼‰
        
        Args:
            chunks: æ–‡æœ¬å—åˆ—è¡¨
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        import json
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(chunks, f, ensure_ascii=False, indent=2)
            logger.info(f"âœ… åˆ‡ç‰‡å·²ä¿å­˜åˆ°: {output_path}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜åˆ‡ç‰‡å¤±è´¥: {e}")
